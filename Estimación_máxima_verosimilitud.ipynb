{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Estimación máxima verosimilitud.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diego-Corro/Autograd_Project/blob/main/Estimaci%C3%B3n_m%C3%A1xima_verosimilitud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psLYIINC0RvI",
        "outputId": "89c53027-d051-4462-97be-70ae98d67302"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import autograd.numpy as np\n",
        "from autograd import grad, jacobian, hessian\n",
        "from autograd.scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import timeit\n",
        "\n",
        "# We worked with the population regression equation y=xβ+ϵ  with a sample of N=5000\n",
        "\n",
        "\n",
        "# number of observations\n",
        "N = 5000\n",
        "# number of parameters\n",
        "K = 10\n",
        "# true parameter values\n",
        "beta = 2 * np.random.randn(K)\n",
        "# true error std deviation\n",
        "sigma =  2\n",
        "\n",
        "def datagen(N, beta, sigma):\n",
        "    \"\"\"\n",
        "    Generates data for OLS regression.\n",
        "    Inputs:\n",
        "    N: Number of observations\n",
        "    beta: K x 1 true parameter values\n",
        "    sigma: std dev of error\n",
        "    \"\"\"\n",
        "    K = beta.shape[0]\n",
        "    x_ = 10 + 2 * np.random.randn(N,K-1)\n",
        "    # x is the N x K data matrix with column of ones\n",
        "    #   in the first position for estimating a constant\n",
        "    x = np.c_[np.ones(N),x_]\n",
        "    # y is the N x 1 vector of dependent variables\n",
        "    y = x.dot(beta) + sigma*np.random.randn(N)\n",
        "    return y, x\n",
        "\n",
        "y, x  = datagen(N, beta, sigma)\n",
        "\n",
        "\n",
        "#now define the logarithmic likelihood function with logpdf:\n",
        "\n",
        "def neg_loglike(theta):\n",
        "    beta = theta[:-1]\n",
        "    # transform theta[-1]\n",
        "    # so that sigma > 0\n",
        "    sigma = np.exp(theta[-1])\n",
        "    mu = np.dot(x,beta)\n",
        "    ll = norm.logpdf(y,mu,sigma).sum()\n",
        "    return -1 * ll\n",
        "\n",
        "#We can then use the jacobian and hessian functions from autograd to evaluate the gradiant and hessian at particular values of our parameters. \n",
        "\n",
        "# derivates of neg_loglike\n",
        "jacobian_  = jacobian(neg_loglike)\n",
        "hessian_ = hessian(neg_loglike)\n",
        "\n",
        "# evaluate the gradiant at true theta\n",
        "# theta = [beta log(sigma)]\n",
        "theta = np.append(beta,np.log(sigma))\n",
        "\n",
        "#We will use the minimize function from scipy for finding the maximum likelihood estimates\n",
        "theta_start = np.append(np.zeros(beta.shape[0]),0.0)\n",
        "res1 = minimize(neg_loglike, theta_start, method = 'BFGS', \\\n",
        "\t       options={'disp': False}, jac = jacobian_)\n",
        "\n",
        "#For looking at the results, we'll calculate standard errors using hessian_ also from autograd and assemble everything in a dataframe for viewing a little later:\n",
        "\n",
        "# estimated parameters\n",
        "theta_autograd = res1.x\n",
        "\n",
        "# for std errors, calculate the information matrix\n",
        "# using the autograd hessian\n",
        "information1 = np.transpose(hessian_(theta_autograd))\n",
        "se1 = np.sqrt(np.diagonal(np.linalg.inv(information1))) \n",
        "\n",
        "# Put Results in a DataFrame\n",
        "results_a = pd.DataFrame({'Parameter':theta_autograd,'Std Err':se1})\n",
        "names = ['beta_'+str(i) for i in range(K)]\n",
        "names.append('log(Sigma)')\n",
        "results_a['Variable'] = names\n",
        "results_a['Model'] = \"MLE Autograd\"\n",
        "\n",
        "print(jacobian_(theta_autograd))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.59553074e-06 5.82786923e-05 5.61638435e-05 5.49269254e-05\n",
            " 4.89675419e-05 5.98813134e-05 5.62509163e-05 5.23701395e-05\n",
            " 5.37076230e-05 5.83045192e-05 9.16751096e-06]\n"
          ]
        }
      ]
    }
  ]
}